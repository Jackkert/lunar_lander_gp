{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolving a Lunar Lander with differentiable Genetic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "To install the required libraries run the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Imports from the standard genepro-multi library are done here. Any adjustments (e.g. different operators) should be made in the notebook. For example:\n",
    "\n",
    "```\n",
    "class SmoothOperator(Node):\n",
    "  def __init__(self):\n",
    "    super(SmoothOperator,self).__init__()\n",
    "    self.arity = 1\n",
    "    self.symb = \"SmoothOperator\"\n",
    "\n",
    "  def _get_args_repr(self, args):\n",
    "    return self._get_typical_repr(args,'before')\n",
    "\n",
    "  def get_output(self, X):\n",
    "    c_outs = self._get_child_outputs(X)\n",
    "    return np.smoothOperation(c_outs[0])\n",
    "\n",
    "  def get_output_pt(self, X):\n",
    "    c_outs = self._get_child_outputs_pt(X)\n",
    "    return torch.smoothOperation(c_outs[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from genepro.node_impl import *\n",
    "from genepro.evo import Evolution\n",
    "from genepro.node_impl import Constant\n",
    "from genepro.variation import subtree_crossover, coeff_mutation\n",
    "from genepro.multitree import Multitree\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "\n",
    "import numpy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Setup\n",
    "Here we first setup the Gymnasium environment. Please see https://gymnasium.farama.org/environments/box2d/lunar_lander/ for more information on the environment. \n",
    "\n",
    "Then a memory buffer is made. This is a buffer in which state transitions are stored. When the buffer reaches its maximum capacity old transitions are replaced by new ones.\n",
    "\n",
    "A frame buffer is initialised used to later store animation frames of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "      self.memory += other.memory\n",
    "      return self \n",
    "\n",
    "    def __add__(self, other):\n",
    "      self.memory = self.memory + other.memory \n",
    "      return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness Function\n",
    "\n",
    "Here you get to be creative. The default setup evaluates 5 episodes of 300 frames. Think of what action to pick and what fitness function to use. The Multi-tree takes an input of $n \\times d$ where $n$ is a batch of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_coeff_mutation(multitree : Multitree, prob_coeff_mut : float= 1, temp : float=0.25) -> Node:\n",
    "  \"\"\"\n",
    "  Applies gaussian coefficient mutations to constant nodes \n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  tree : Node\n",
    "    the tree to which coefficient mutations are applied\n",
    "  prob_coeff_mut : float, optional\n",
    "    the probability with which coefficients are mutated (default is 0.25)\n",
    "  temp : float, optional\n",
    "    \"temperature\" that indicates the strength of coefficient mutation, it is relative to the current value (i.e., v' = v + temp*abs(v)*N(0,1))\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  Node\n",
    "    the tree after coefficient mutation (it is the same as the tree in input)\n",
    "  \"\"\"\n",
    "  r = np.random.randint(multitree.n_trees)\n",
    "  tree = multitree.children[r]\n",
    "  coeffs = [n for n in tree.get_subtree() if type(n) == Constant]\n",
    "  for c in coeffs:\n",
    "    # decide wheter it should be applied\n",
    "    if numpy.random.uniform(0, 1) < prob_coeff_mut:\n",
    "      if not hasattr(c, 'sigma'):\n",
    "          sample = numpy.random.normal(0, 0.1**2)\n",
    "          setattr(c, 'sigma', max(math.exp(sample), 10**(-16)))\n",
    "      else: \n",
    "          v = c.get_value()\n",
    "          # update the value by +- temp relative to current value\n",
    "          # sample = random.gauss(0, 0.1)\n",
    "          new_v = numpy.random.normal(v, c.sigma**2)\n",
    "    \n",
    "          c.sigma = max(c.sigma * math.exp(numpy.random.normal(0, 0.1**2)), 10**(-16))\n",
    "          # new_v = v + sample #temp*np.abs(v)*randn()\n",
    "          c.set_value(new_v)\n",
    "  \n",
    "  multitree.children[r] = tree\n",
    "  return multitree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_coeff_opt(multitree : Multitree, optimize : bool, prob_coeff_mut : float= 1, temp : float=0.25, ) -> Node:\n",
    "\n",
    "    if not optimize:\n",
    "        return multitree\n",
    "        \n",
    "    batch_size = 128\n",
    "    GAMMA = 0.99\n",
    "    \n",
    "    constants = multitree.get_subtrees_consts()\n",
    "    \n",
    "    if len(constants)>0:\n",
    "      optimizer = optim.AdamW(constants, lr=1e-3, amsgrad=True)\n",
    "    \n",
    "    for _ in range(500):\n",
    "    \n",
    "      if len(constants)>0 and len(evo.memory)>batch_size:\n",
    "        target_tree = copy.deepcopy(multitree)\n",
    "    \n",
    "        transitions = evo.memory.sample(batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), dtype=torch.bool)\n",
    "    \n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                   if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "        state_action_values = multitree.get_output_pt(state_batch).gather(1, action_batch)\n",
    "        next_state_values = torch.zeros(batch_size, dtype=torch.float)\n",
    "        with torch.no_grad():\n",
    "          next_state_values[non_final_mask] = target_tree.get_output_pt(non_final_next_states).max(1)[0].float()\n",
    "    \n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "        \n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "       \n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(constants, 100)\n",
    "        optimizer.step()\n",
    "\n",
    "    return multitree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_function_pt(multitree, num_episodes=5, episode_duration=300, render=False, ignore_done=False):\n",
    "  memory = ReplayMemory(10000)\n",
    "  rewards = []\n",
    "\n",
    "  for _ in range(num_episodes):\n",
    "    # get initial state of the environment\n",
    "    observation = env.reset()\n",
    "    observation = observation[0]\n",
    "    \n",
    "    for _ in range(episode_duration):\n",
    "      if render:\n",
    "        frames.append(env.render())\n",
    "\n",
    "      input_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "      \n",
    "      action = torch.argmax(multitree.get_output_pt(input_sample))\n",
    "      observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "      rewards.append(reward)\n",
    "      output_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "      memory.push(input_sample, torch.tensor([[action.item()]]), output_sample, torch.tensor([reward]))\n",
    "      if (terminated or truncated) and not ignore_done:\n",
    "        break\n",
    "\n",
    "  fitness = np.sum(rewards) / num_episodes\n",
    "  \n",
    "  return fitness, memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution Setup\n",
    "Here the leaf and internal nodes are defined. Think about the odds of sampling a constant in this default configurations. Also think about any operators that could be useful and add them here. \n",
    "\n",
    "Adjust the population size (multiple of 8 if you want to use the standard tournament selection), max generations and max tree size to taste. Be aware that each of these settings can increase the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = env.observation_space.shape[0]\n",
    "leaf_nodes = [Feature(i) for i in range(num_features)]\n",
    "leaf_nodes = leaf_nodes + [Constant()] # Think about the probability of sampling a coefficient\n",
    "internal_nodes = [Plus(),Minus(),Times(),Div(),Square(),Sqrt()] #Add your own operators here\n",
    "\n",
    "for i in range 10\n",
    "evo = Evolution(\n",
    "  fitness_function_pt, internal_nodes, leaf_nodes,\n",
    "  4,\n",
    "  pop_size=48,\n",
    "  max_gens=40,\n",
    "  max_tree_size=31,\n",
    "  coeff_opts = [{\"fun\":DQN_coeff_opt, \"rate\": 1}],\n",
    "  n_jobs=8,\n",
    "  verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolve\n",
    "Running this cell will use all the settings above as parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen: 1,\tbest of gen fitness: -71.586,\tbest of gen size: 27\n",
      "gen: 2,\tbest of gen fitness: -32.456,\tbest of gen size: 27\n",
      "gen: 3,\tbest of gen fitness: -38.480,\tbest of gen size: 27\n",
      "gen: 4,\tbest of gen fitness: -36.274,\tbest of gen size: 31\n",
      "gen: 5,\tbest of gen fitness: -33.627,\tbest of gen size: 31\n"
     ]
    }
   ],
   "source": [
    "evo.evolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "import pickle\n",
    "\n",
    "file_path = 'best_gp.pickle'\n",
    "\n",
    "# Open the file in binary mode\n",
    "with open(file_path, 'wb') as file:\n",
    "    # Serialize and write the variable to the file\n",
    "    pickle.dump(evo.best_of_gens[-1], file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_score(tree):\n",
    "    rewards = []\n",
    "\n",
    "    for i in range(10):\n",
    "      # get initial state\n",
    "      observation = env.reset(seed=i)\n",
    "      observation = observation[0]\n",
    "\n",
    "      for _ in range(500):    \n",
    "        # build up the input sample for GP\n",
    "        input_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "        # get output (squeezing because it is encapsulated in an array)\n",
    "        output = tree.get_output_pt(input_sample)\n",
    "        action = torch.argmax(tree.get_output_pt(input_sample))\n",
    "        observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "        rewards.append(reward)\n",
    "\n",
    "\n",
    "        output_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "        if (terminated or truncated):\n",
    "            break\n",
    "\n",
    "    fitness = np.sum(rewards)\n",
    "    \n",
    "    return fitness / 10\n",
    "\n",
    "best = evo.best_of_gens[-1]\n",
    "\n",
    "# best = gaussian_coeff_mutation(best, 1)\n",
    "\n",
    "print(best.get_readable_repr())\n",
    "print(\"test score:\")\n",
    "print(get_test_score(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make an animation\n",
    "Here the best evolved individual is selected and one episode is rendered. Make sure to save your lunar landers over time to track progress and make comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "# gist to save gif from https://gist.github.com/botforge/64cbb71780e6208172bbf03cd9293553\n",
    "def save_frames_as_gif(frames, path='./', filename='evolved_lander.gif'):\n",
    "  plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "  patch = plt.imshow(frames[0])\n",
    "  plt.axis('off')\n",
    "  def animate(i):\n",
    "      patch.set_data(frames[i])\n",
    "  anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "  anim.save(path + filename, writer='imagemagick', fps=60)\n",
    "\n",
    "frames = []\n",
    "fitness_function_pt(best, num_episodes=1, episode_duration=500, render=True, ignore_done=False)\n",
    "env.close()\n",
    "save_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"evolved_lander.gif\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "The coefficients in the multi-tree aren't optimised. Here Q-learning (taken from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) is used to optimise the weights further. Incorporate coefficient optimisation in training your agent(s). Coefficient Optimisation can be expensive. Think about how often you want to optimise, when, which individuals etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Bring more elaborated ideas (also taking inspiration from the scientific literature) about how the provided GP library could be improved using coefficient optimisation. Optimisation can be expensive, so consider what individuals or what portion of the population should be optimised. Important: At this stage, each idea should be formalised into a motivating hypothesis (e.g., “Assuming that . . . happens, then we expect an improvement by doing . . . ”.\n",
    "\n",
    "---\n",
    "\n",
    "## Jack\n",
    "\n",
    "The optimization can be done periodically while evolving as well. For example: optimizing the best multitree coefficients of every generation, or every x generations.\n",
    "\n",
    "Some potentially useful papers\n",
    "1. Genetic Programming and Reinforcement Learning on Learning Heuristics for Dynamic Scheduling: A Preliminary Comparison (https://www.researchgate.net/publication/380254508_Genetic_Programming_and_Reinforcement_Learning_on_Learning_Heuristics_for_Dynamic_Scheduling_A_Preliminary_Comparison)\n",
    "2. Reinforced Genetic Programming (https://link.springer.com/article/10.1023/A:1011953410319)\n",
    "3. Multi-modal multi-objective model-based genetic programming to find multiple diverse high-quality models (https://arxiv.org/pdf/2203.13347)\n",
    "4. Multi-objective Genetic Programming Optimization of Decision Trees for Classifying Medical Data (https://link.springer.com/chapter/10.1007/978-3-540-45224-9_42)\n",
    "5. Coefficient Mutation in the Gene-pool Optimal Mixing Evolutionary Algorithm for Symbolic Regression (https://dl.acm.org/doi/pdf/10.1145/3520304.3534036)\n",
    "\n",
    "Mind here: programs in this case = multitrees\n",
    "\n",
    "Ideas:\n",
    "1. Optimizing coefficients of the best multitree of each generation during evolution. Assuming that we optimize the coefficients of the best individual in a population, increasing the fitness, its offspring will also be fitter. We expect an improvement in \"convergence rate\" (fitness improves faster). This does not affect the global structure of the trees but the coefficients get \"propagated\"\n",
    "2. The same, but not the best of every generation to save compute.\n",
    "3. Optimize the coefficients on the worst individuals. Assume that the fitness increases to the point that the program will be included in the tournament selection. Then, this program is \"saved\" from being eliminated, and will perhaps evolve into a good (or best) solution. This is kind of like exploration, avoiding a pure greedy approach.\n",
    "4. Same reasoning for optimizing the number 2 or 3, or multiple programs, but that will be very expensive so probably not good.\n",
    "5. Inspired by paper: Coefficient Mutation in the Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA). Use Gaussian coefficient mutation to mutate the coefficients to allow for more exploration, and fine-tuning solutions. This could be combined with optimizing while evolving for example. E.g. optimizing the best program and mutating it using gaussian coefficient mutation to create new programs. Then see if these programs are among the highest fitness to be selected by tournament selection. Assuming that this propagates the best solution while also exploring the solution (program) space, then this will ultimately lead to an overall better solution.\n",
    "6. Inspired by: Multi-objective Genetic Programming Optimization of Decision Trees for Classifying Medical Data. Use this technique that they are using to enhance their GP's coefficient optimization properties. But right now I don't have enough time to figure out what they did. ### TODO: LOOK AT PAPER. They state this: \"We have incorporated a Quasi-Newton optimization technique to augment the\n",
    "power ofthe GP coefficient optimization. This technique uses an error propaga-\n",
    "tion algorithm that efficiently calculates the gradient ofthe error function with\n",
    "respect to the coefficients embedded in the GP expression tree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "constants = best.get_subtrees_consts()\n",
    "\n",
    "if len(constants)>0:\n",
    "  optimizer = optim.AdamW(constants, lr=1e-3, amsgrad=True)\n",
    "\n",
    "for _ in range(500):\n",
    "\n",
    "  if len(constants)>0 and len(evo.memory)>batch_size:\n",
    "    target_tree = copy.deepcopy(best)\n",
    "\n",
    "    transitions = evo.memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                        batch.next_state)), dtype=torch.bool)\n",
    "\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                               if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = best.get_output_pt(state_batch).gather(1, action_batch)\n",
    "    next_state_values = torch.zeros(batch_size, dtype=torch.float)\n",
    "    with torch.no_grad():\n",
    "      next_state_values[non_final_mask] = target_tree.get_output_pt(non_final_next_states).max(1)[0].float()\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "   \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(constants, 100)\n",
    "    optimizer.step()\n",
    "\n",
    "print(best.get_readable_repr())\n",
    "print(get_test_score(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "fitness_function_pt(best, num_episodes=1, episode_duration=500, render=True, ignore_done=False)\n",
    "env.close()\n",
    "save_frames_as_gif(frames, filename='evolved_lander_RL.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"evolved_lander_RL.gif\" width=\"750\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
